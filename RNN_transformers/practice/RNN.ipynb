{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import string\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Подготовка данных\n",
    "\n",
    "Загружаем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем количество встречаемости каждого токена из датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_count = {}\n",
    "\n",
    "for text in tqdm(dataset[\"train\"][\"text\"]):\n",
    "    text_clean = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    for token in word_tokenize(text):\n",
    "        token = token.strip()\n",
    "        token_to_count[token] = token_to_count.get(token, 0) + 1\n",
    "        \n",
    "print(f\"Кол-во токенов в словаре: {len(token_to_count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим специальные токены:\n",
    "- \\<unk\\>: неизвестный токен;\n",
    "- \\<bos\\>: начало последовательности;\n",
    "- \\<eos\\>: конец последовательности;\n",
    "- \\<pad\\>: специальный токен для объединения последовательностей разных длин в один батч.\n",
    "\n",
    "А также, удалим редкие токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set([\"<unk>\", \"<bos>\", \"<eos>\", \"<pad>\"])\n",
    "\n",
    "threshold = 25\n",
    "for token, token_count in tqdm(token_to_count.items()):\n",
    "    if token_count >= threshold:\n",
    "        vocabulary.add(token)\n",
    "    \n",
    "print(f\"Кол-во токенов в словаре: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_index = {token: index for index, token in enumerate(vocabulary)}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заведем специальный класс для хранения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    \n",
    "    def __init__(self, text: list[str], label: list[int]) -> None:\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        \n",
    "        self.unk_id = token_to_index[\"<unk>\"]\n",
    "        self.bos_id = token_to_index[\"<bos>\"]\n",
    "        self.eos_id = token_to_index[\"<eos>\"]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        text  = self.text[index].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        label = self.label[index]\n",
    "\n",
    "        text_indices = [self.bos_id]\n",
    "        text_indices = text_indices + [token_to_index.get(x, self.unk_id) for x in word_tokenize(text)]\n",
    "        text_indices = text_indices + [self.eos_id]\n",
    "        \n",
    "        return (text_indices, label)\n",
    "    \n",
    "\n",
    "def collate_fn(batch: list[list[int]], max_length: int = 256) -> torch.Tensor:\n",
    "    max_length = min(max_length, max([len(item[0]) for item in batch]))\n",
    "    \n",
    "    batch_text_indices = []\n",
    "    for item in batch:\n",
    "        text_indices = item[0][:max_length]\n",
    "        for _ in range(max_length - len(text_indices)):\n",
    "            text_indices.append(token_to_index[\"<pad>\"])\n",
    "        batch_text_indices.append(text_indices)\n",
    "    \n",
    "    batch_labels = [item[1] for item in batch]\n",
    "    \n",
    "    return (torch.LongTensor(batch_text_indices), torch.LongTensor(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = np.random.choice(np.arange(dataset[\"test\"].num_rows), 2000)\n",
    "\n",
    "train_dataset = CustomDataset(dataset[\"train\"][\"text\"], dataset[\"train\"][\"label\"])\n",
    "valid_dataset = CustomDataset(\n",
    "    dataset[\"test\"].select(valid_indices)[\"text\"],\n",
    "    dataset[\"test\"].select(valid_indices)[\"label\"]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True,  collate_fn=collate_fn, batch_size=128)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=collate_fn, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RecurrentNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        hidden_size: int, \n",
    "        output_size: int,\n",
    "        vocabulary_size: int, \n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: float = False\n",
    "    ) -> None:\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocabulary_size, input_size)\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers,\n",
    "            # nonlinearity=\"tanh\",\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Linear(2 * hidden_size if self.bidirectional else hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        output, _ = self.rnn(embedding)\n",
    "        output = output.max(dim=1)[0]\n",
    "        output = self.dense(output)\n",
    "                \n",
    "        # output = self.dense(output[:, -1, :])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = RecurrentNeuralNetwork(\n",
    "    input_size=128, \n",
    "    hidden_size=128, \n",
    "    output_size=2, \n",
    "    vocabulary_size=len(token_to_index),\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_index['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = {\"train\": [], \"valid\": []}\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    for x, y in tqdm(train_loader, desc=\"Train\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        y_train_true.extend(y.tolist())\n",
    "        y_train_pred.extend(output.argmax(dim=1).tolist())\n",
    "    losses[\"train\"].append(train_loss / len(train_loader))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(valid_loader, desc=\"Valid\"):\n",
    "            output = model(x)\n",
    "            y_valid_true.extend(y.tolist())\n",
    "            y_valid_pred.extend(output.argmax(dim=1).tolist())\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "    losses[\"valid\"].append(valid_loss / len(valid_loader))\n",
    "    \n",
    "    print(f\"train roc_auc_score={roc_auc_score(y_train_true, y_train_pred)}, valid roc_auc_score={roc_auc_score(y_valid_true, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-search-proj-6jnEKmKf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
